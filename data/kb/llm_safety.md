Large Language Model (LLM) safety evaluations commonly include prompt injection tests,
data leakage checks, output toxicity assessments, and hallucination detection.
Regression test suites help maintain safety guarantees across model updates.
