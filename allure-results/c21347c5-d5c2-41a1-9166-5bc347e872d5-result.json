{"name": "Query returns correct golden answer -- @1.3 ", "status": "failed", "statusDetails": {"message": "AssertionError: Expected 'William Shakespeare', got 'Here is a brief answer.'\n", "trace": "  File \"C:\\Users\\asusv\\OneDrive\\Desktop\\ai-agent-test-framework\\.venv\\Lib\\site-packages\\behave\\model.py\", line 1991, in run\n    match.run(runner.context)\n    ~~~~~~~~~^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\asusv\\OneDrive\\Desktop\\ai-agent-test-framework\\.venv\\Lib\\site-packages\\behave\\matchers.py\", line 105, in run\n    self.func(context, *args, **kwargs)\n    ~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"features\\steps\\golden_steps.py\", line 9, in step_impl\n    assert similarity > 0.9, f\"Expected '{expected}', got '{answer}'\"\n           ^^^^^^^^^^^^^^^^\n"}, "steps": [{"name": "Given the API is available", "status": "passed", "start": 1755086000016, "stop": 1755086000021}, {"name": "When I ask \"Who wrote Hamlet?\"", "status": "passed", "start": 1755086000022, "stop": 1755086000027}, {"name": "Then the response should match \"William Shakespeare\"", "status": "failed", "statusDetails": {"message": "AssertionError: Expected 'William Shakespeare', got 'Here is a brief answer.'\n", "trace": "  File \"C:\\Users\\asusv\\OneDrive\\Desktop\\ai-agent-test-framework\\.venv\\Lib\\site-packages\\behave\\model.py\", line 1991, in run\n    match.run(runner.context)\n    ~~~~~~~~~^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\asusv\\OneDrive\\Desktop\\ai-agent-test-framework\\.venv\\Lib\\site-packages\\behave\\matchers.py\", line 105, in run\n    self.func(context, *args, **kwargs)\n    ~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"features\\steps\\golden_steps.py\", line 9, in step_impl\n    assert similarity > 0.9, f\"Expected '{expected}', got '{answer}'\"\n           ^^^^^^^^^^^^^^^^\n"}, "start": 1755086000027, "stop": 1755086000029}], "parameters": [{"name": "query", "value": "Who wrote Hamlet?"}, {"name": "expected", "value": "William Shakespeare"}], "start": 1755086000015, "stop": 1755086000031, "uuid": "d612f05f-cad5-40a6-aaf8-d52ce56e4342", "historyId": "cde3ec2a3dcbcbe1cf8d3c7d1e419ac2", "fullName": "Golden Answer Validation: Query returns correct golden answer", "labels": [{"name": "severity", "value": "normal"}, {"name": "feature", "value": "Golden Answer Validation"}, {"name": "framework", "value": "behave"}, {"name": "language", "value": "cpython3"}], "titlePath": ["features", "Golden Answer Validation"]}